---
title: Preregistration of the Analyses for the Study on Justified vs. Unjustified
  Anger Prototypes
date: "`r Sys.Date()`"
output:
  word_document: 
    toc: yes
    toc_depth: 3
    reference_docx: word-doc-template.docx
  html_document: default
  pdf_document:
    toc: yes
    toc_depth: 3
    highlight: yes
editor_options:
  chunk_output_type: console
---

\newpage

# Study Procedure

After providing consent, participants are randomly assigned to one of two conditions: justified and non-justified anger. First, they receive instructions to think about a time in the past that they experienced anger. Depending on the condition, they are directed to consider an experience where their anger was justified or unjustified.
After recalling and writing about a past experience of anger, participants complete a series of questionnaires about the various aspects of their experience. The full instructions and measures, including the set of items relevant to these analyses are provided in a separate document.

# Sample

The data collection for this study started in September 2021, and is currently in progress. None of the following analyses have been tested on the actual dataset. The development and planning of the following hypotheses, research questions, and preregistered analyses are data-independent. These analyses will be conducted once the target sample size of 1054 is reached. 

## Sample Size Determination

Since the majority of analyses consist of comparisons of two independent samples, the target sample size was determined with the goal of achieving 90% power (with α = .05) to detect a small-to-moderate between-groups difference (i.e., Cohen’s *d* = 0.20; Lovakov & Agadullina, 2021). A priori power analysis using the *pwr* package in R (Champely, 2020) indicated that this goal can be achieved with a minimum sample size of 1054 participants. 

## Data Exclusion

Prior to the analyses, participants' anger narratives will be reviewed. Participant data will be excluded from further analyses if:
* No anger narrative is provided.
* The response provided does not follow the instructions. For example, instead of discussing a past experience of anger, the response refers to an irrelevant experience (e.g., a past experience of happiness) or the participant's personal theory about anger (e.g., "Anger is really bad.").
* The response provides more than one experience, and it is not clear which one is the focal experience for which the participant has responded to the rest of the items.


```{r data-exclusion, eval=FALSE}

#set up categorical variables
df <- df %>% 
          mutate(NarrativeWritten = as.factor(df$NarrativeWritten),
                 NarrativeRelevant = as.factor(df$NarrativeRelevant),
                 Condition = as.factor(df$Condition))

#assigning values to factor levels
  levels(df$NarrativeWritten) <- c("No", "Yes")

  levels(df$NarrativeRelevant) <- c("No", "Yes", NA) 

  levels(df$Condition) <- c("justified", "nonjustified", NA)

#drop cases based on exclusion criteria
  df1 <- df %>% 
    filter(NarrativeWritten == "Yes") %>% 
    filter(NarrativeRelevant == "Yes") %>% 
    filter(!is.na(Condition))


knitr::opts_chunk$set(echo = TRUE)
```

# Hypotheses, Research Questions, and Analyses Plan


## Perceptions of the Target's Behavior 

### Harm and threat

**Hypothesis 1:** Justified anger events (compared to the unjustified ones) involve a stronger perception of harm and threat to the self and to others.

**Hypothesis 2:** The difference between justified and unjustified anger in terms of perceptions of harm or threat is moderated by the target of harm, such that this difference is larger for harm to others (vs. harm to self). 

```{r, harm-threat, eval=FALSE}
#First, examine the correlation between the 2 items for threat/harm to self and the 2 items for threat/harm to other. If r > .5, proceed with the following analyses (i.e., creating composite scores by averaging the 2 items for each variable). If not, conduct these analyses for each item separately.

  cor.test(df1$harm_you, df1$threat_you) 
  cor.test(df1$harm_others, df1$threat_others) 
  
#assuming items are strongly correlated (r > .5)

  #create the dataframe
  harm_threat_df <- df1 %>% 
                        select(ResponseId, Condition, harm_you, harm_others, threat_you, threat_others) %>% 
                        mutate(harm.threat_self = ((harm_you + threat_you) / 2),
                               harm.threat_other = ((harm_others + threat_others) / 2))
  
  #create a long dataframe
  harm_threat_df_long <- harm_threat_df %>% 
                              tidyr::pivot_longer(cols = c(harm.threat_self, harm.threat_other),
                                                  names_to = "target",
                                                  names_prefix = "harm.threat_",
                                                  values_to = "harm")

#the model: 
  #the main effect of type of anger (i.e., condition) corresponds to Hypothesis 1
  #the interaction effect corresponds to Hypothesis 2
  
  harm_threat_df_long %>% 
    rstatix::anova_test(data = .,
                        dv = harm,
                        wid = ResponseId,
                        between = Condition,
                        within = target) %>% 
    rstatix::get_anova_table()


#The difference between the two conditions in terms of harm/threat to self:
  t.test(harm.threat_self ~ Condition, data = harm_threat_df)
  effectsize::cohens_d(harm.threat_self ~ Condition, data = harm_threat_df)

#The difference between the two conditions in terms of harm/threat to others:
  t.test(harm.threat_other ~ Condition, data = harm_threat_df)
  effectsize::cohens_d(harm.threat_other ~ Condition, data = harm_threat_df)

```


### Fairness and Justification

**Hypothesis:** The target's behavior is perceived as less fair and less justifiable in the justified (vs. unjustified) anger condition.

```{r, fair-justified, eval=FALSE}
#First, examine the correlation between the 2 items. If r > .5, proceed with the following analyses. If not, conduct these analyses for each item separately.

  cor.test(df1$beh_fair, df1$beh_justified) 

#assuming items are strongly correlated (r > .5) 
  df1 <- df1 %>% 
            mutate(fair_just = ((beh_fair + beh_justified) / 2))
  
  df1 %>% 
      t.test(fair_just ~ Condition, data = .)
  
  df1 %>% 
      effectsize::cohens_d(fair_just ~ Condition, data = .)

```

### Norm Violations

**Hypothesis 1:** The target’s behavior during justified anger events (vs. unjustified ones) is judged as a stronger norm violation. 

**Hypothesis 2:** The difference between justified and unjustified anger events is moderated by the type of norm violation, such that this difference is larger for injunctive norms (compared to descriptive norms). 

```{r, norms, eval=FALSE}
#create the dataframe
    norms_df <- df1 %>% 
                      select(ResponseId, Condition, how_acceptable, how_common) %>% 
                      mutate(injunctive = 6 - how_acceptable,
                             descriptive = 6 - how_common)
                      
                      

#create a long dataframe
    norms_df_long <- norms_df %>% 
                            tidyr::pivot_longer(cols = c(injunctive, descriptive),
                                                names_to = "norm_type",
                                                values_to = "violation")

#The model: 
    #The main effect of type of anger (i.e., condition) corresponds to Hypothesis 1
    #The interaction effect corresponds to Hypothesis 2
    
    norms_df_long %>% 
      rstatix::anova_test(data = .,
                          dv = violation,
                          wid = ResponseId,
                          between = Condition,
                          within = norm_type) %>% 
      rstatix::get_anova_table()


#The difference between the two conditions in terms of injunctive norms
    t.test(injunctive ~ Condition, data = norms_df)
    effectsize::cohens_d(injunctive ~ Condition, data = norms_df)

#The difference between the two conditions in terms of descriptive norms
  t.test(descriptive ~ Condition, data = norms_df)
  effectsize::cohens_d(descriptive ~ Condition, data = norms_df)

```

## Perceptions of the Target 

### Causal Attribution

**Hypothesis:** In anger events that are perceived as justified (compared to the unjustified ones), participants are more likely to consider the cause of anger to stem from the target’s internal and stable characteristics (as opposed to the external and changeable circumstances).

```{r, causal-attr, eval=FALSE}
#First, test the correlation between the two items: 
cor.test(df1$cause_circumst, df1$behave_same)

#If r > .5, create a composite score and compare conditions:
    df1 <- df1 %>% 
              mutate(causal_attr = ((cause_circumst + behave_same) / 2))
    df1 %>% 
        t.test(causal_attr ~ Condition, data = .)
    df1 %>% 
        effectsize::cohens_d(causal_attr ~ Condition, data = .)

#If r < .5, compare conditions for each item separately:
  #cause: internal vs. external 
    df1 %>% 
        t.test(cause_circumst ~ Condition, data = .)
    df1 %>% 
        effectsize::cohens_d(cause_circumst ~ Condition, data = .)
  #cause: stable vs. the same 
    df1 %>% 
        t.test(behave_same ~ Condition, data = .)
    df1 %>% 
        effectsize::cohens_d(behave_same ~ Condition, data = .)
```

### Moral Character

**Hypothesis:** In anger events perceived as justified (compared to the unjustified ones), the target is more likely to be seen as having weaker moral and ethical values. 

```{r, moral-char, eval=FALSE}
#create the morality composite score using the 10 items from Walker & Pitts (1998) 

  df_morality <- df1 %>% 
                    select(ResponseId, Condition, starts_with("wp_"), gw_honest, gw_principled)

  keys.list <- list(highly_moral=c("wp_concerned_right","wp_faithful","wp_clear_values", "wp_lawabiding", 
                                   "wp_strong_beliefs", "wp_distinguishes", "wp_dev_conscience", "wp_ethical", 
                                   "gw_honest", "gw_principled"))

  morality_scores <- psych::scoreItems(keys.list, df_morality)
  morality_scores
  df_morality <- cbind(df_morality, morality_scores$scores)

#Compare conditions
    df_morality %>% 
        t.test(highly_moral ~ Condition, data = .)
    df_morality %>% 
        effectsize::cohens_d(highly_moral ~ Condition, data = .)

```

### Moral-Relational Judgments

**Research Question:** What are the differences between justified vs. unjustified anger in terms of the moral-relational judgments of the target (as measured using Goodwin et al.'s [2014] trait characteristic items)? <br>

Goodwin et al. (2014) demonstrate that judgments of morality and warmth are separable and can provide unique informational value for person perception. To examine the perceptions of the target along these two critical dimensions, dimensionality reduction analysis (i.e., PCA) will be conducted on the character judgments from Goodwin et al. (2014). <br>

The number of components will be decided based on the scree plot, parallel analysis, and Velicor's MAP. If multiple alternative solutions are suggested by these methods, PCA will be conducted for all the alternatives number of components, and the optimal solution will be chosen based on component interpretability. <br>


```{r, moral-relational-pca, eval=FALSE}
df_moral_relational <- df1 %>% 
                    select(ResponseId, Condition, starts_with("gw_"))

#To make a decision about the number of components
  #scree plot
  df_moral_relational %>% 
    select(starts_with("gw_")) %>% 
    psych::scree(hline = -1)
  
  #parallel analysis
  df_moral_relational %>% 
    select(starts_with("gw_")) %>% 
    psych::fa.parallel()
  
  #Velicor's MAP
  df_moral_relational %>% 
    select(starts_with("gw_")) %>% 
    psych::nfactors(n = 32)


#PCA with n components (n will be decided based on the outcome of analyses above)
  pca_n_component <- df_moral_relational %>% 
                        select(starts_with("gw_")) %>%  
                        psych::principal(., nfactors = n, rotate = "varimax") 
        
  n_comp_outcome <- psych::kaiser(pca_n_component, rotate = "Varimax") %>% psych::fa.sort()

  n_comp_outcome[["loadings"]] %>% 
                    knitr::kable(digits = 2) %>%
                    kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>% 
                    kableExtra::kable_paper(full_width = F)

  #If there are cross-loading items (i.e., items that have high loadings on more than one component) in the PCA outcome, in an iterative process, such items will be excluded one-by-one (starting with the ones that have the strongest cross-loading) and PCA will be conducted with the reduced number of items until an interpretable solution without cross-loading is found. 

```

Based on the PCA results, scale scores will be created by averaging the corresponding items for each component. Next, the mean of the emerging components will be compared between the two conditions. <br>

_Note:_ Considering the data-driven nature of the analyses below, and the primary focus on discovery, Benjamin et al.'s (2018) recommendations to reduce the rate of false positives will be applied. Namely, the significance threshold will be set at _p_ < .005, and the analysis results with .05 > _p_s ≥ .005 will be reported as “suggestive”.

```{r, moral-relational-compare, eval=FALSE}

#Hypothetical scenario: code for calculating scale scores for 2 components, each having 4 indicators
  keys.list <- list(component1=c("...","...","...","..."),
                    component2=c("...","...","...","..."))

  moral_relation_scores <- psych::scoreItems(keys.list, df_moral_relational)
  moral_relation_scores
  df_moral_relational <- cbind(df_moral_relational, moral_relation_scores$scores)


#Hypothetical scenario: compare conditions for each of the 2 hypothetical components (see above)
  #component1
    df_moral_relational %>% 
        t.test(component1 ~ Condition, data = .)
    df_moral_relational %>% 
        effectsize::cohens_d(component1 ~ Condition, data = .)

  #component2
    df_moral_relational %>% 
        t.test(component2 ~ Condition, data = .)
    df_moral_relational %>% 
        effectsize::cohens_d(component2 ~ Condition, data = .)
```


## Experiencer's Emotional Processes

### Affective Experience

**Research Question:** What are the differences between justified vs. unjustified anger in terms of the intensity of affective experiences? <br>

To compare the broader affective experiences of participants across the justified and unjustified anger events, participants are asked to rate their emotional experience during the anger eliciting event using 27 positive and negative emotions. These emotions cover diverse experiences such as hostility, self-blame, social fears, and boldness/empowerment, and have been previously used to evaluate affective experiences in response to moral-relational violations (Razavi et al., 2022). Using these emotion items, the differences in emotional experiences associated with the two anger variants will be examined. <br>

First, the structure of the affective experiences will be determined using dimensionality reduction analysis (i.e., PCA). <br>

The number of components will be decided based on the scree plot, parallel analysis, and Velicor's MAP. If multiple alternative solutions are suggested by these methods, PCA will be conducted for all the alternatives number of components, and the optimal solution will be chosen based on component interpretability. <br>

```{r, affec-pca, eval=FALSE}
df_affect <- df1 %>% 
                    select(ResponseId, Condition, starts_with("em_"))

  #scree plot
  df_affect %>% 
    select(starts_with("em_")) %>% 
    psych::scree(hline = -1)
  
  #parallel analysis
  df_affect %>% 
    select(starts_with("em_")) %>% 
    psych::fa.parallel()
  
  #Velicor's MAP
  df_affect %>% 
    select(starts_with("em_")) %>% 
    psych::nfactors(n = 27)


#PCA with n components (n will be replaced based on the outcome of analyses above)
  pca_n_component <- df_affect %>% 
                        select(starts_with("em_")) %>%  
                        psych::principal(., nfactors = n, rotate = "varimax") 
        
  n_comp_outcome <- psych::kaiser(pca_n_component, rotate = "Varimax") %>% psych::fa.sort()

  n_comp_outcome[["loadings"]] %>% 
                    knitr::kable(digits = 2) %>%
                    kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>% 
                    kableExtra::kable_paper(full_width = F)

  #If there are cross-loading items (i.e., items that have high loadings on more than one component) in the PCA outcome, in an iterative process, such items will be excluded one-by-one (starting with the ones that have the strongest cross-loading) and PCA will be conducted with the reduced number of items until an interpretable solution without cross-loading is found. 

```

Next, scale scores will be created by averaging the corresponding items for each component. These scores will be used to compare the intensity of emotion categories between the two conditions. 

_Note:_ Considering the data-driven nature of these analyses and the focus on discovery, Benjamin et al.'s (2018) recommendations to reduce the rate of false positives will be followed. Namely, the significance threshold will be set at _p_ < .005, and the analysis results with .05 > _p_s ≥ .005 will be reported as “suggestive”.

```{r, affect-compare, eval=FALSE}

#Hypothetical scenario: code for calculating scale scores for 2 components
  keys.list <- list(component1=c("...","...","...","..."),
                    component2=c("...","...","...","..."))

  affect_scores <- psych::scoreItems(keys.list, df_affect)
  affect_scores
  df_affect <- cbind(df_affect, affect_scores$scores)

#hypothetical scenario: compare conditions for each of the 2 components
  #component1
    df_affect %>% 
        t.test(component1 ~ Condition, data = .)
    df_affect %>% 
        effectsize::cohens_d(component1 ~ Condition, data = .)

  #component2
    df_affect %>% 
        t.test(component2 ~ Condition, data = .)
    df_affect %>% 
        effectsize::cohens_d(component2 ~ Condition, data = .)
```


### Expressivity

**Hypothesis 1:** Participants are more likely to evaluate their own anger expression as “exaggerated” in the unjustified (vs. justified) anger condition.

```{r, exag-express, eval=FALSE}
df1$behav_reac <- as.factor(df1$behav_reac)
levels(df1$behav_reac) <- c("fully_concealed", "partly_concealed", "fully_expressed", "exaggerated")

  #A function to calculate percentages for each category of a Factor variable
    percentage <- function(var, includeNA = TRUE) {
          tabb <- table(var) %>% as.data.frame()
              if (includeNA == TRUE) {
                   tabb$percentage <- (tabb$Freq * 100 / length(var))
              } else {
                    tabb$percentage <- (tabb$Freq * 100 / sum(tabb$Freq))
              }
          colnames(tabb)[1] <- c("category")
          print(tabb)
    }

  percentage(df1$behav_reac)

#Compare the reactions based on Condition
  #cross-tabs
    expressivity_table1 <- xtabs( ~ Condition + behav_reac,
                               data=df1)
    prop.table(expressivity_table1, 1) %>% 
                                    round(2) 
  #overall chi-square
    chisq.test(df1$Condition, 
               df1$behav_reac)

  #If the overall chi-square is significant, conduct these follow-up analyses. Of these four analyses, the first one corresponds to the hypothesis (stated above), and the rest are exploratory:
  df_exaggerated <- df1 %>% 
                        mutate(exaggerated = if_else(behav_reac == "exaggerated", "Yes", "No")) %>% 
                        select(Condition, exaggerated)
    chisq.test(df_exaggerated$Condition, 
               df_exaggerated$exaggerated)
    
  df_fully_expressed <- df1 %>% 
                        mutate(fully_expressed = if_else(behav_reac == "fully_expressed", "Yes", "No")) %>% 
                        select(Condition, fully_expressed)
    chisq.test(df_fully_expressed$Condition, 
               df_fully_expressed$fully_expressed)  
    
  df_partly_concealed <- df1 %>% 
                        mutate(partly_concealed = if_else(behav_reac == "partly_concealed", "Yes", "No")) %>% 
                        select(Condition, partly_concealed)
    chisq.test(df_partly_concealed$Condition, 
               df_partly_concealed$partly_concealed)
    
  df_fully_concealed <- df1 %>% 
                        mutate(fully_concealed = if_else(behav_reac == "fully_concealed", "Yes", "No")) %>% 
                        select(Condition, fully_concealed)
    chisq.test(df_fully_concealed$Condition, 
               df_fully_concealed$fully_concealed)
    
```


**Hypothesis 2:** The strength of the experience-expression relation tamps down as the anger intensity increases, and this deceleration is stronger for justified anger. 

```{r, feel-exp-relation, eval=FALSE}
df_feel_exp <- df1 %>% 
                select(ResponseId, Condition, anger_feel, anger_express) %>% 
                mutate(anger_feel_c = as.double(scale(anger_feel, scale = FALSE)),
                       anger_feel_c_sq = anger_feel_c^2)

#A linear model testing the feel-express relation
  mod1_linear <- lm(anger_express ~ anger_feel_c,
                    data = df_feel_exp)
  summary(mod1_linear)

#A curvilinear model testing the feel-express relation: This is the test of the first part of the hypothesis. 
 
  mod1_nonlinear <- lm(anger_express ~ anger_feel_c * anger_feel_c_sq,
                    data = df_feel_exp)
  summary(mod1_nonlinear)
  anova(mod1_linear, mod1_nonlinear)

#If the curvilinear relation is significant, the moderating role of Condition for this curvilinear relation will be tested, which will correspond to the second part of the hypothesis.   
  mod2_nonlinear <- lm(anger_express ~ anger_feel_c * anger_feel_c_sq * Condition,
                    data = df_feel_exp)
  summary(mod2_nonlinear)

```

## Experiencer's Relational Processes

### Perception of Target's Regret/Apology

**Research Question:** Do the perceptions of the target as regretful and apologetic vary depending on whether they consider the anger event as justified or unjustified? 

```{r, regret-apol, eval=FALSE}
  #Regret 
    df1 %>% 
        t.test(person_regret ~ Condition, data = .)
    df1 %>% 
        effectsize::cohens_d(person_regret ~ Condition, data = .)
  
  #Apology
    df1 %>% 
        t.test(person_apologize ~ Condition, data = .)
    df1 %>% 
        effectsize::cohens_d(person_apologize ~ Condition, data = .)

```

### Relationship Closeness

**Hypothesis 1:** Relationship closeness deteriorates following an anger event, both “short-term” (i.e., comparing closeness before and immediately after the event) and “long-term” (i.e., comparing closeness before the event and at the time of the study).

```{r closeness1, eval=FALSE}
closeness_df <- df1 %>% 
                      select(ResponseId, Condition, prior_closeness, immed_closeness, now_closeness)

#create a long dataframe
closeness_df_long <- closeness_df %>% 
                            tidyr::pivot_longer(cols = c(prior_closeness, immed_closeness, now_closeness),
                                                names_to = "time",
                                                values_to = "closeness") %>% 
                            mutate(time = as.factor(time)) %>% 
                            mutate(time = forcats::fct_relevel(time, "prior_closeness", "immed_closeness", "now_closeness"))
                      
  levels(closeness_df_long$time) <- c("prior", "immediate", "now")

#setup the contrasts
  contrasts(closeness_df_long$Condition) <- contr.sum
  contrasts(closeness_df_long$time) <- contr.sum  

#Overal model
  closeness_model <- lmer(closeness ~ time * Condition + (1 | ResponseId),
                          data = closeness_df_long)

  anova(closeness_model, type = "III") %>% knitr::kable()

#comparing closeness (a) before vs. immediately after the event 
  pairwise_mod <- emmeans::emmeans(closeness_model, pairwise ~ time)
  pairwise_mod[["contrasts"]][1]

#comparing closeness (b) before vs. now
  pairwise_mod[["contrasts"]][2]

```

**Hypothesis 2:** The long-term deterioration of relationship closeness is stronger for justified (vs. unjustified) anger experiences.

```{r, closeness2, eval=FALSE}
longterm_mod <- emmeans::emmeans(closeness_model, pairwise ~ time | Condition)
  
  #justified: prior vs. now
  longterm_mod[["contrasts"]][2]
  #not justified: prior vs. now
  longterm_mod[["contrasts"]][5]

```

## Predictors of Anger Experience

**Research Question 1:** What is the unique association between various characteristics of the anger eliciting event/person (i.e., perception of harm/threat to self and to others, perception of fairness/justification, perception of target’s regret, perception of target's tendency to apologize, causal attributions, norm violations, relationship closeness prior to the event) and participants’ anger experience?

```{r, pred-experience, eval=FALSE}
#Assuming items for each of these composites are strongly correlated (r > .5), composite scores will be created by averaging: 
df1 <- df1 %>% 
          mutate(fair_just = ((beh_fair + beh_justified) / 2),
                 harm.threat_self = ((harm_you + threat_you) / 2),
                  harm.threat_other = ((harm_others + threat_others) / 2),
                 causal_attr = ((cause_circumst + behave_same) / 2)) 

  #If the items constituting each of the composites above do not meet the assumption of high correlation, they will be entered into the model individually (instead of being entered as a composite score).

  #reverse-scoring the norm items
  df1 <- df1 %>% 
              mutate(injunctive = 6 - how_acceptable,
                      descriptive = 6 - how_common)

#Testing for multicolinearity
  #check the correlation matrix for r > .9
    df1 %>% 
      select(harm.threat_self, harm.threat_other, fair_just, 
             person_regret, person_apologize, causal_attr, injunctive, descriptive, prior_closeness) %>% 
      cor(use = "pairwise.complete.obs") %>% 
      round(2)

  #check for tolerance < .1 and VIFs > 5
    feel_model <- lm(anger_feel ~ harm.threat_self + harm.threat_other + fair_just + person_regret + person_apologize + causal_attr + injunctive + descriptive + prior_closeness,
                     data = df1)

    olsrr::ols_vif_tol(feel_model)
    
  #if any of the three criteria above is violated, evaluate the model and address the issue by dropping predictor(s) that are problematic. If no multicolinearity issue is detected, proceed to interpreting the model outcome:
    
    summary(feel_model)
    
```

**Research Question 2:** To what extent do the predictors of anger intensity (based on the model above) vary depending on whether the anger event is perceived as justified or unjustified?

```{r, pred-experience-mod, eval=FALSE}
feel_model_interaction <- lm(anger_feel ~ harm.threat_self * Condition + harm.threat_other * Condition + fair_just * Condition + person_regret * Condition + person_apologize * Condition + causal_attr * Condition + injunctive * Condition + descriptive * Condition + prior_closeness * Condition,
                     data = df1)

  summary(feel_model_interaction)

```

## Predictors of Anger Expression

**Research Question 1:** What is the unique association between various characteristics of the anger eliciting event/person (i.e., perception of harm/threat to self and to others, perception of fairness/justification, perception of target’s regret, perception of target's tendency to apologize, causal attributions, norm violations, relationship closeness prior to the event) and participants’ anger expression?

```{r, pred-express, eval=FALSE}
#Note: Creating composite scores will follow the same plan as explained in the previous section (i.e., Predictors of Anger Experience). The following models will be adjusted if/when the assumption of high correlation between composite items is not met.

  #Testing for multicolinearity: check for tolerance < .1 and VIFs > 5
    express_model <- lm(anger_express ~ harm.threat_self + harm.threat_other + fair_just + person_regret + person_apologize + causal_attr + injunctive + descriptive + prior_closeness,
                     data = df1)

    olsrr::ols_vif_tol(express_model)
    
  #If any of the multicolinearity criteria is violated, evaluate the model and address the issue by dropping predictor(s) that are problematic. If no multicolinearity issue is detected, proceed to interpreting the model outcome:
    
    summary(express_model)
```

**Research Question 2:** To what extent do the predictors of anger expressivity (based on the model above) vary depending on whether the anger event is perceived as justified or unjustified?

```{r, pred-express-mod, eval=FALSE}
express_model_interaction <- lm(anger_express ~ harm.threat_self * Condition + harm.threat_other * Condition + fair_just * Condition + person_regret * Condition + person_apologize * Condition + causal_attr * Condition + injunctive * Condition + descriptive * Condition + prior_closeness * Condition,
                     data = df1)

  summary(express_model_interaction)

```

## Detecting Justified vs. Unjustified Anger

**Research Question:** Of the various characteristics of the anger eliciting event and person (i.e., perception of harm/threat to self and to others, perception of fairness/justification, perception of target’s regret, perception of target's tendency to apologize, causal attributions, norm violations, relationship closeness prior to the event), which ones are significant classifiers of anger events into justified vs. unjustified  categories?

```{r classifi, eval=FALSE}

logistic_model <- glm(Condition ~ harm.threat_self + harm.threat_other + fair_just + person_regret + person_apologize + causal_attr + injunctive + descriptive + prior_closeness, 
             family = "binomial", data = df1)
  #Model summary provides asnwer to the RQ above.
    summary(logistic_model)

#Additional analyses to descriptively examine the model performance and importance of the predictors    
  #McFadden’s R^2
    pscl::pR2(logistic_model)["McFadden"]

  #Variable importance
    caret::varImp(logistic_model, scale = FALSE) %>% 
      arrange(desc(Overall))
  
  #How did the model do?
    predicted <- predict(logistic_model, df1, type = "response")
    
    df1$condition_numeric <- ifelse(df1$Condition == "justified", 0, 1)
  
  #confusion matrix [shows predictions compared to the actual defaults]
    InformationValue::confusionMatrix(df1$condition_numeric, predicted)
  #sensitivity (the “true positive rate”)
    InformationValue::sensitivity(df1$condition_numeric, predicted)
  #specificity (the “true negative rate”)
    InformationValue::specificity(df1$condition_numeric, predicted)
  #ROC  
    InformationValue::plotROC(df1$condition_numeric, predicted)
```
